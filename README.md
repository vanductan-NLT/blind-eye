<div align="center">
<img width="1200" height="475" alt="Blind Eye Banner" src="https://github.com/user-attachments/assets/0aa67016-6eaf-458a-adb2-6e31a0763ed6" />

# üëÅÔ∏è Blind Eye

### AI-Powered Visual Companion for the Visually Impaired

[![Google Gemini](https://img.shields.io/badge/Powered%20by-Google%20Gemini-4285F4?style=for-the-badge&logo=google&logoColor=white)](https://ai.google.dev/)
[![React](https://img.shields.io/badge/React-18-61DAFB?style=for-the-badge&logo=react&logoColor=white)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5-3178C6?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Vite](https://img.shields.io/badge/Vite-5-646CFF?style=for-the-badge&logo=vite&logoColor=white)](https://vitejs.dev/)

</div>

---

## üåü Overview

**Blind Eye** is a real-time AI-powered visual companion designed to assist visually impaired individuals in navigating their environment safely and independently. Using cutting-edge Google Gemini AI models, the app acts as a trusted friend walking beside the user, providing natural, conversational guidance about their surroundings.

### üéØ Key Features

| Feature | Description |
|---------|-------------|
| **üö∂ Live Navigation Mode** | Continuous real-time guidance using camera feed. Warns about obstacles, stairs, and hazards using clock-face directions (e.g., "Chair at 2 o'clock, 3 steps away"). |
| **üé§ Voice Assistant (Ask AI)** | Voice-activated Q&A. Ask anything: "What's in front of me?", "Read this sign", "Is the path clear?" |
| **üß† Intelligent Model Routing** | Automatically selects the optimal AI model - Flash for quick responses, Gemini 3 Pro for complex analysis like reading documents. |
| **üìç Location-Aware** | Integrates with device GPS for context-aware navigation assistance. |
| **üîä Text-to-Speech** | Clear, natural voice feedback for all guidance. |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        BLIND EYE APP                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Camera     ‚îÇ  ‚îÇ  Microphone  ‚îÇ  ‚îÇ    GPS Location      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (WebRTC)   ‚îÇ  ‚îÇ  (Web API)   ‚îÇ  ‚îÇ    (Geolocation)     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                      ‚îÇ              ‚îÇ
‚îÇ         ‚ñº                 ‚ñº                      ‚ñº              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                    React Frontend                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ App.tsx (Main Controller)                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ HUD Component (User Interface)                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ useSpeechRecognition Hook                              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                              ‚îÇ                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                    Services Layer                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ geminiService   ‚îÇ ‚îÇ   liveClient    ‚îÇ ‚îÇspeechService ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ Model Router  ‚îÇ ‚îÇ ‚Ä¢ Real-time API ‚îÇ ‚îÇ ‚Ä¢ TTS Output ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ Smart Assist  ‚îÇ ‚îÇ ‚Ä¢ Audio I/O     ‚îÇ ‚îÇ              ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ Navigation    ‚îÇ ‚îÇ ‚Ä¢ Video Stream  ‚îÇ ‚îÇ              ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ              ‚îÇ                   ‚îÇ                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                   ‚îÇ
               ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GOOGLE GEMINI AI                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ gemini-2.5-flash ‚îÇ  ‚îÇ gemini-3-pro    ‚îÇ  ‚îÇ Gemini Live API ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (Fast responses) ‚îÇ  ‚îÇ (Deep analysis) ‚îÇ  ‚îÇ (Real-time)     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Getting Started

### Prerequisites

- **Node.js** v18 or higher
- **Google Gemini API Key** ([Get one here](https://makersuite.google.com/app/apikey))
- Modern browser with camera and microphone access

### Installation

```bash
# Clone the repository
git clone https://github.com/vanductan-NLT/blind-eye.git
cd blind-eye

# Install dependencies
npm install

# Configure API key
cp .env.local.example .env.local
# Edit .env.local and add your GEMINI_API_KEY

# Start development server
npm run dev
```

### Environment Variables

Create a `.env.local` file in the project root:

```env
GEMINI_API_KEY=your_api_key_here
```

---

## üì± Usage

### Live Navigation Mode
1. Click **"START LIVE"** button
2. Point your camera forward as you walk
3. Receive continuous guidance like:
   - *"Path is clear, keep going straight."*
   - *"Stop! Stairs going down right in front of you."*
   - *"Chair at 2 o'clock, 3 steps away. Bear left."*

### Voice Assistant Mode
1. Click **"ASK AI"** button
2. Speak your question naturally
3. Examples:
   - "What's in front of me?"
   - "Read this sign"
   - "Is the path clear?"
   - "What color is this?"

---

## üß† AI Model Strategy

| Task Type | Model Used | Reason |
|-----------|------------|--------|
| Quick identification | `gemini-2.5-flash` | Speed priority |
| Document reading | `gemini-3-pro-preview` | Accuracy priority |
| Live navigation | `gemini-2.5-flash` | Real-time performance |
| Complex reasoning | `gemini-3-pro-preview` | Deep analysis |

The app uses an intelligent router that analyzes each query and automatically selects the optimal model.

---

## üõ†Ô∏è Tech Stack

- **Frontend**: React 18, TypeScript, Vite
- **AI**: Google Gemini API, Gemini Live API
- **Speech**: Web Speech API (Recognition + Synthesis)
- **Camera**: WebRTC, react-webcam
- **Styling**: Tailwind CSS

---

## üìÅ Project Structure

```
blind-eye/
‚îú‚îÄ‚îÄ App.tsx                 # Main application component
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ HUD.tsx             # User interface overlay
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îî‚îÄ‚îÄ useSpeechRecognition.ts  # Voice input hook
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ geminiService.ts    # AI analysis & routing
‚îÇ   ‚îú‚îÄ‚îÄ liveClient.ts       # Real-time Gemini Live API
‚îÇ   ‚îú‚îÄ‚îÄ speechService.ts    # Text-to-speech output
‚îÇ   ‚îî‚îÄ‚îÄ audioUtils.ts       # Audio processing utilities
‚îú‚îÄ‚îÄ types.ts                # TypeScript definitions
‚îî‚îÄ‚îÄ vite.config.ts          # Build configuration
```

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **[Blind-Nav Android App](https://github.com/knightafter/Blind-Nav-Android-App-for-Blind-persons)** - The original inspiration for this project. Our web implementation builds upon the concepts and approaches pioneered by this excellent Android application.
- **Google Gemini** for powering the AI vision and language capabilities
- **Google AI Studio** for the development environment
- All contributors and testers who helped improve accessibility

## üí° Inspiration

This project was inspired by and references the **[Blind-Nav Android App for Blind Persons](https://github.com/knightafter/Blind-Nav-Android-App-for-Blind-persons)** by [@knightafter](https://github.com/knightafter). 

We adapted the core navigation concepts for a web-based experience, making it accessible across all platforms without requiring app installation. Key inspiration includes:
- Real-time camera frame analysis for navigation
- Continuous voice guidance approach
- Priority-based obstacle detection (hazards > obstacles > path)
- Natural, conversational AI responses

---

<div align="center">

**Made with ‚ù§Ô∏è for accessibility**

*Empowering independence for the visually impaired*

[View in AI Studio](https://ai.studio/apps/drive/1_XfGi2NxDB6oOnBmmaHuCudS4AH5PMQB) ‚Ä¢ [Report Bug](https://github.com/vanductan-NLT/blind-eye/issues) ‚Ä¢ [Request Feature](https://github.com/vanductan-NLT/blind-eye/issues)

</div>
